{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb のコピー",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1iAto1V1p2CVfBVUY1Xl-pxZCSthfVMF0",
      "authorship_tag": "ABX9TyNQBDQeh3F8kd0V2V3EPyT4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vdslab/bearinforest_graph_analysis/blob/main/swem/swem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"/content/2021.json\") as f:\n",
        "  raw = json.load(f)\n",
        "  data = raw[\"articles\"]\n",
        "  print(data)\n",
        "  for record in data:\n",
        "    print(record[\"title\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "Y7XyNWj6NJsm",
        "outputId": "2e58726e-4a3a-49b4-87a7-5f1b7632489f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bbdeff613618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"articles\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'length'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import json\n",
        "\n",
        "\n",
        "class SWEM():\n",
        "    \"\"\"\n",
        "    Simple Word-Embeddingbased Models (SWEM)\n",
        "    https://arxiv.org/abs/1805.09843v1\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, w2v, tokenizer, oov_initialize_range=(-0.01, 0.01)):\n",
        "        self.w2v = w2v\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = set(self.w2v.vocab.keys())\n",
        "        self.embedding_dim = self.w2v.vector_size\n",
        "        self.oov_initialize_range = oov_initialize_range\n",
        "\n",
        "        if self.oov_initialize_range[0] > self.oov_initialize_range[1]:\n",
        "            raise ValueError(\"Specify valid initialize range: \"\n",
        "                             f\"[{self.oov_initialize_range[0]}, {self.oov_initialize_range[1]}]\")\n",
        "\n",
        "    def get_word_embeddings(self, text):\n",
        "        np.random.seed(abs(hash(text)) % (10 ** 8))\n",
        "\n",
        "        vectors = []\n",
        "        for word in self.tokenizer.tokenize(text):\n",
        "            if word in self.vocab:\n",
        "                vectors.append(self.w2v[word])\n",
        "            else:\n",
        "                vectors.append(np.random.uniform(self.oov_initialize_range[0],\n",
        "                                                 self.oov_initialize_range[1],\n",
        "                                                 self.embedding_dim))\n",
        "        return np.array(vectors)\n",
        "\n",
        "    def average_pooling(self, text):\n",
        "        word_embeddings = self.get_word_embeddings(text)\n",
        "        return np.mean(word_embeddings, axis=0)\n",
        "\n",
        "    def max_pooling(self, text):\n",
        "        word_embeddings = self.get_word_embeddings(text)\n",
        "        return np.max(word_embeddings, axis=0)\n",
        "\n",
        "    def concat_average_max_pooling(self, text):\n",
        "        word_embeddings = self.get_word_embeddings(text)\n",
        "        return np.r_[np.mean(word_embeddings, axis=0), np.max(word_embeddings, axis=0)]\n",
        "\n",
        "    def hierarchical_pooling(self, text, n):\n",
        "        word_embeddings = self.get_word_embeddings(text)\n",
        "\n",
        "        text_len = word_embeddings.shape[0]\n",
        "        if n > text_len:\n",
        "            raise ValueError(f\"window size must be less than text length / window_size:{n} text_length:{text_len}\")\n",
        "        window_average_pooling_vec = [np.mean(word_embeddings[i:i + n], axis=0) for i in range(text_len - n + 1)]\n",
        "\n",
        "        return np.max(window_average_pooling_vec, axis=0)\n",
        "\n",
        "print(0)\n",
        "tokenizer =tfds.deprecated.text.Tokenizer()\n",
        "#googleの学習済みw2vモデルを使ってる\n",
        "w2v_path = \"/content/drive/MyDrive/GoogleNews-vectors-negative300.bin\"\n",
        "print(1)\n",
        "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)\n",
        "print(2)\n",
        "swem = SWEM(w2v, tokenizer)\n",
        "print(3)\n",
        "\n",
        "vector_ave_data = []\n",
        "vector_max_data = []\n",
        "vector_cat_data = []\n",
        "vector_hier_data = []\n",
        "with open(\"/content/drive/MyDrive/2021.json\") as f:\n",
        "  raw = json.load(f)\n",
        "  data = raw[\"articles\"]\n",
        "  \n",
        "  for record in data:\n",
        "    vector_ave_data.append({'title':record['title'], 'vector':swem.average_pooling( record['abstract'] ) .tolist() })\n",
        "    vector_max_data.append({'title':record['title'], 'vector':swem.max_pooling(record['abstract'] ).tolist() })\n",
        "    vector_cat_data.append({'title':record['title'], 'vector':swem.concat_average_max_pooling(record['abstract'] ).tolist() })\n",
        "    #vector_hier_data.append({'title':record['title'], 'vector':swem.hierarchical_pooling( n = 2, text = record['abstract'] ).tolist() })\n",
        "\n",
        "with open('/content/2021_swem_ave_sim.json', 'w') as f:\n",
        "    json.dump(vector_ave_data, f, indent=4)\n",
        "\n",
        "with open('/content/2021_swem_max_sim.json', 'w') as f:\n",
        "    json.dump(vector_max_data, f, indent=4)\n",
        "\n",
        "with open('/content/2021_swem_cat_sim.json', 'w') as f:\n",
        "    json.dump(vector_cat_data, f, indent=4)\n",
        "\n",
        "#with open('/content/2021_swem_hier_sim.json', 'w') as f:\n",
        "#    json.dump(vector_hier_data, f, indent=4)\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GMkQEcOIgZI",
        "outputId": "67fbe9bf-0534-42a8-fcb1-aefb53e6a37a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import csv\n",
        "import random\n",
        "\n",
        "\n",
        "data = []\n",
        "\n",
        "path = '/content/drive/MyDrive/2021_swem_cat_sim.json'\n",
        "with open(path) as f:\n",
        "  raw = json.load(f)\n",
        "\n",
        "  for record in raw:\n",
        "    data.append({\"title\":record['title'], \"vector\":np.array(record['vector'])})\n",
        "\n",
        "length = 100\n",
        "minv = 2.0\n",
        "maxv = -2.0\n",
        "\n",
        "#dataからlength個ランダムにサンプリングする\n",
        "sid = random.sample(range(len(data)), length)\n",
        "\n",
        "with open('/content/node_2021_swem_max.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(['Id', 'Label'])\n",
        "\n",
        "    for i in range(length):\n",
        "      writer.writerow([sid[i], data[sid[i]]['title']])\n",
        "\n",
        "with open('/content/edge_2021_swem_max.csv', 'w') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Source\",\"Target\",\"Cosine_similarity\"])\n",
        "\n",
        "    for i in range(length):\n",
        "      for j in range(length):\n",
        "        if(i != j):\n",
        "          a = data[sid[i]][\"vector\"]\n",
        "          b = data[sid[j]][\"vector\"]\n",
        "          cosim = a @ b  / (np.linalg.norm(a) * np.linalg.norm(b) )\n",
        "          writer.writerow([sid[i], sid[j], cosim])\n",
        "          \n",
        "          minv = min(minv, cosim)\n",
        "          maxv = max(maxv, cosim)\n",
        "print(minv)\n",
        "print(maxv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RPC_qaTZAPN",
        "outputId": "77ed3a70-2d35-44f7-a769-d0cbfac79200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6640051241624468\n",
            "1.0\n"
          ]
        }
      ]
    }
  ]
}